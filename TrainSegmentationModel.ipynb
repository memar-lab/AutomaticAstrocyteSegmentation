{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54cd5459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T23:18:52.519789Z",
     "start_time": "2023-08-03T23:18:52.515466Z"
    }
   },
   "outputs": [],
   "source": [
    "####### Inputs #######\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "DEFAULT_LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 200\n",
    "model_list = [\"UnetPlusPlus\"]\n",
    "backbone_list = [\"vgg19\"]\n",
    "# Other options for networks (model) and backbone\n",
    "# model_list = [\"UnetPlusPlus\", \"Unet\", \"MAnet\", \"Linknet\", \"PSPNet\", \"FPN\"]\n",
    "# #backbone_list = [\"vgg16\", \"vgg19\", \"resnet50\", \"resnet101\", \"resnet152\", \"mobilenet_v2\", \"efficientnet-b4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787adbf-d287-4c7a-b78b-a356d396b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from numpy import ndarray\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from torchinfo import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# Takes in a .png and returns a 2D numpy array of 0's and 1's\n",
    "\n",
    "def image2nparray(image_file):\n",
    "    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "    image_mask = ma.make_mask(image, copy=True)\n",
    "    array_out = np.array(image_mask, dtype=int)\n",
    "    return array_out\n",
    "\n",
    "# Takes a numpy array prediction from the AI and returns png\n",
    "\n",
    "def nparray2image(nparray, filename, directory):\n",
    "    name = filename.split('.')\n",
    "    image_name = name[0] + \"_pred.png\"\n",
    "    \n",
    "    path = os.path.join(directory, image_name)\n",
    "    \n",
    "    cv2.imwrite(path, nparray*255)\n",
    "    image_out = cv2.imread(path)\n",
    "    return image_out\n",
    "\n",
    "# Concatenates and saves two images:\n",
    "    # 1. Takes prediction image and overlays it on the raw image\n",
    "    # 2. Takes manually labeled image and overlays it on the raw image\n",
    "\n",
    "def pred2comp(pred, raw, labeled, filename, directory):\n",
    "\n",
    "    # print(\"raw shape:\", raw.shape)\n",
    "    # print(\"pred shape:\", pred.shape)\n",
    "    # print(\"labeled shape:\", labeled.shape)\n",
    "\n",
    "    pred_overlay = cv2.addWeighted(raw, 0.8, pred, 0.8, 0.0)\n",
    "\n",
    "    orig_overlay = cv2.addWeighted(raw, 0.8, labeled, 0.8, 0.0)\n",
    "    \n",
    "    height = np.shape(raw)[0]\n",
    "    buffer = np.ones((height, 5, 3), dtype=np.uint8)*255\n",
    "    combined = np.hstack((raw, buffer, orig_overlay, buffer, pred_overlay)) \n",
    "\n",
    "    name = filename.split('.')\n",
    "    image_name = name[0] + \"_comp.png\"\n",
    "    path = os.path.join(directory, image_name)\n",
    "\n",
    "    cv2.imwrite(path, combined)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Creates the training and testing data sets for our three different methods which are 'Control', 'Random', and 'Triple'\n",
    "# Method to run is selected in the next cell \n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, raw_folder, label_folder):\n",
    "        if model_to_run == 'Triple':     \n",
    "            self.raw_images_list = os.listdir(raw_folder)\n",
    "            self.raw_images_dir = raw_folder\n",
    "            self.labeled_images = os.listdir(label_folder)\n",
    "            self.labeled_images_dir = label_folder\n",
    "\n",
    "            # Creating transform attributes\n",
    "            #self.raw_normalize = transforms.Normalize(mean = [0.0839, 0.0857, 0.0868], std = [0.1734, 0.1740, 0.1746])\n",
    "            self.jitter = transforms.ColorJitter(brightness = 0.25, contrast = 0.4)\n",
    "            self.flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "            self.to_tensor = transforms.ToTensor()\n",
    " \n",
    "        elif model_to_run == 'Random':\n",
    "            self.raw_images_list = os.listdir(raw_folder)\n",
    "            self.raw_images_dir = raw_folder\n",
    "            self.labeled_images = os.listdir(label_folder)\n",
    "            self.labeled_images_dir = label_folder\n",
    "\n",
    "            # Creating transform attributes\n",
    "            #self.raw_normalize = transforms.Normalize(mean = [0.0839, 0.0857, 0.0868], std = [0.1734, 0.1740, 0.1746])\n",
    "            self.jitter = transforms.ColorJitter(brightness = 0.25, contrast = 0.4)\n",
    "            self.flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "            self.combined = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness = 0.25, contrast = 0.4),\n",
    "                transforms.RandomHorizontalFlip(p=1.0)])\n",
    "            self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        elif model_to_run == 'Control':\n",
    "            self.raw_images_list = os.listdir(raw_folder)\n",
    "            self.raw_images_dir = raw_folder\n",
    "            self.labeled_images = os.listdir(label_folder)\n",
    "            self.labeled_images_dir = label_folder\n",
    "\n",
    "            # Creating transform attributes\n",
    "            #self.raw_normalize = transforms.Normalize(mean = [0.0839, 0.0857, 0.0868], std = [0.1734, 0.1740, 0.1746])\n",
    "            # (mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) this is the original\n",
    "            # Corrected means and standard deviations are: mean=[0.0839, 0.0857, 0.0868], std=[0.1734, 0.1740, 0.1746]\n",
    "            self.to_tensor = transforms.ToTensor()\n",
    "            \n",
    "    def __len__(self):\n",
    "        # size (length) of the dataset\n",
    "        if model_to_run == 'Triple':\n",
    "            return (len(self.raw_images_list)) *3\n",
    "        else:\n",
    "            return len(self.raw_images_list)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if model_to_run == 'Triple': \n",
    "            # Select the \"index\"th item from the dataset\n",
    "            # Will return the item in the row \"index\" in self.details\n",
    "            true_length = len(self.raw_images_list)\n",
    "            category = index // true_length\n",
    "            item_data = {}\n",
    "\n",
    "            if category == 0:\n",
    "                # proceed as normal, so original image\n",
    "                orig_filename = self.raw_images_list[index]\n",
    "                item_data['filename'] = orig_filename\n",
    "                item_data['categoryname'] = orig_filename\n",
    "            \n",
    "                raw_image = os.path.join(self.raw_images_dir, orig_filename)\n",
    "                raw_image_data = cv2.imread(raw_image)\n",
    "                raw_image_data = self.to_tensor(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "\n",
    "                label_dir = os.path.join(self.labeled_images_dir, orig_filename)\n",
    "                label_image_data = cv2.imread(label_dir)\n",
    "                label_image_data = self.to_tensor(label_image_data)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = image2nparray(label_dir)\n",
    "\n",
    "            elif category == 1:\n",
    "                # Apply color jitter\n",
    "                index = index % true_length\n",
    "\n",
    "                orig_filename = self.raw_images_list[index]\n",
    "                item_data['filename'] = orig_filename\n",
    "                start_name = orig_filename.split('.')\n",
    "                new_name = start_name[0] + \"_coljit.png\"\n",
    "                item_data['categoryname'] = new_name\n",
    "        \n",
    "                raw_image = os.path.join(self.raw_images_dir, orig_filename)\n",
    "                raw_image_data = cv2.imread(raw_image)\n",
    "                raw_image_data = self.to_tensor(raw_image_data)\n",
    "                raw_image_data = self.jitter(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "\n",
    "                label_dir = os.path.join(self.labeled_images_dir, orig_filename)\n",
    "                label_image_data = cv2.imread(label_dir)\n",
    "                label_image_data = self.to_tensor(label_image_data)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = image2nparray(label_dir)\n",
    "\n",
    "            else:\n",
    "                # RandomHorizontalFlip\n",
    "                index = index % true_length\n",
    "\n",
    "                orig_filename = self.raw_images_list[index]\n",
    "                item_data['filename'] = orig_filename\n",
    "                start_name = orig_filename.split('.')\n",
    "                new_name = start_name[0] + \"_hflip.png\"\n",
    "                item_data['categoryname'] = new_name\n",
    "            \n",
    "                raw_image = os.path.join(self.raw_images_dir, orig_filename)\n",
    "                raw_image_data = cv2.imread(raw_image)\n",
    "                raw_image_data = self.to_tensor(raw_image_data)\n",
    "                raw_image_data = self.flip(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "\n",
    "                label_dir = os.path.join(self.labeled_images_dir, orig_filename)\n",
    "                label_image_data = cv2.imread(label_dir)\n",
    "                label_image_data = self.to_tensor(label_image_data)\n",
    "                label_image_mask = image2nparray(label_dir)\n",
    "                label_image_mask = self.to_tensor(label_image_mask)\n",
    "                label_image_data = self.flip(label_image_data)\n",
    "                label_image_mask = self.flip(label_image_mask)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = torch.squeeze(label_image_mask)\n",
    "\n",
    "                # For Debugging\n",
    "                # assert item_data['labeled'].shape == item_data['raw'][:, :, 0].shape, \"The label width and height does not match the raw image\"\n",
    "        \n",
    "        elif model_to_run == 'Random': \n",
    "            # Select the \"index\"th item from the dataset\n",
    "            # Will return the item in the row \"index\" in self.details\n",
    "            random_number = random.random()\n",
    "\n",
    "            item_data = {}\n",
    "            orig_filename = self.raw_images_list[index]\n",
    "            item_data['filename'] = orig_filename\n",
    "        \n",
    "            raw_image = os.path.join(self.raw_images_dir, orig_filename)\n",
    "            raw_image_data = cv2.imread(raw_image)\n",
    "            raw_image_data = self.to_tensor(raw_image_data)\n",
    "\n",
    "            label_dir = os.path.join(self.labeled_images_dir, orig_filename)\n",
    "            label_image_data = cv2.imread(label_dir)\n",
    "            label_image_data = self.to_tensor(label_image_data)\n",
    "            label_image_mask = image2nparray(label_dir)\n",
    "\n",
    "            # If statement to randomly apply transforms\n",
    "            if random_number < 0.25:\n",
    "                # Original/ no transforms\n",
    "                item_data['categoryname'] = orig_filename\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = label_image_mask\n",
    "            elif random_number < 0.50:\n",
    "                # ColorJitter only\n",
    "                start_name = orig_filename.split('.')\n",
    "                new_name = start_name[0] + \"_coljit.png\"\n",
    "                item_data['categoryname'] = new_name\n",
    "                raw_image_data = self.jitter(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = label_image_mask\n",
    "            elif random_number < 0.75:\n",
    "                # HorizontalFlip only\n",
    "                start_name = orig_filename.split('.')\n",
    "                new_name = start_name[0] + \"_hflip.png\"\n",
    "                item_data['categoryname'] = new_name\n",
    "                raw_image_data = self.flip(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "                label_image_mask = self.to_tensor(label_image_mask)\n",
    "                label_image_data = self.flip(label_image_data)\n",
    "                label_image_mask = self.flip(label_image_mask)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = torch.squeeze(label_image_mask)\n",
    "            else:\n",
    "                # Both\n",
    "                start_name = orig_filename.split('.')\n",
    "                new_name = start_name[0] + \"_both.png\"\n",
    "                item_data['categoryname'] = new_name\n",
    "                raw_image_data = self.combined(raw_image_data)\n",
    "                item_data['rawimg'] = raw_image_data\n",
    "                item_data['raw'] = raw_image_data\n",
    "                # item_data['raw'] = self.raw_normalize(raw_image_data)\n",
    "                label_image_mask = self.to_tensor(label_image_mask)\n",
    "                label_image_data = self.flip(label_image_data)\n",
    "                label_image_mask = self.flip(label_image_mask)\n",
    "                item_data['labelimg'] = label_image_data\n",
    "                item_data['labeled'] = torch.squeeze(label_image_mask)\n",
    "        elif model_to_run == 'Control':\n",
    "            # Select the \"index\"th item from the dataset\n",
    "            # Will return the item in the row \"index\" in self.details\n",
    "\n",
    "            item_data = {}\n",
    "            orig_filename = self.raw_images_list[index]\n",
    "            item_data['filename'] = orig_filename\n",
    "        \n",
    "            raw_image = os.path.join(self.raw_images_dir, orig_filename)\n",
    "            raw_image_data = cv2.imread(raw_image)\n",
    "            raw_image_data = self.to_tensor(raw_image_data)     \n",
    "            item_data['rawimg'] = raw_image_data\n",
    "            item_data['raw'] = raw_image_data\n",
    "            # item_data['raw'] = self.raw_normalize(raw_image_data)  \n",
    "\n",
    "            label_dir = os.path.join(self.labeled_images_dir, orig_filename)\n",
    "            label_image_data = cv2.imread(label_dir)\n",
    "            item_data['labelimg'] = self.to_tensor(label_image_data)\n",
    "            item_data['labeled'] = image2nparray(label_dir)\n",
    "        \n",
    "            # For Debugging\n",
    "            # assert item_data['labeled'].shape == item_data['raw'][:, :, 0].shape, \"The label width and height does not match the raw image\"\n",
    "        \n",
    "        # print(\"Shape of labeled:\", item_data['labeled'].shape)\n",
    "        # print(\"Shape of labelimg:\", item_data['labelimg'].shape)\n",
    "        # print(\"Shape of raw:\", item_data['raw'].shape)\n",
    "        # print(\"Shape of rawimg:\", item_data['rawimg'].shape)\n",
    "        \n",
    "\n",
    "\n",
    "            # Get current sizes\n",
    "            h, w = item_data['labeled'].shape  # labeled is still 2D at this point\n",
    "\n",
    "            # Compute required padding\n",
    "            pad_h = (32 - (h % 32)) % 32\n",
    "            pad_w = (32 - (w % 32)) % 32\n",
    "\n",
    "            # Split padding between top/bottom and left/right\n",
    "            pad_top = pad_h // 2\n",
    "            pad_bottom = pad_h - pad_top\n",
    "            pad_left = pad_w // 2\n",
    "            pad_right = pad_w - pad_left\n",
    "\n",
    "            # Save padding info for later uncropping\n",
    "            item_data['pad'] = {\n",
    "                'top': pad_top,\n",
    "                'bottom': pad_bottom,\n",
    "                'left': pad_left,\n",
    "                'right': pad_right,\n",
    "                'orig_hw': (h, w)  # optional but handy for sanity checks\n",
    "            }\n",
    "            # Apply padding to label (NumPy)\n",
    "            item_data['labeled'] = np.pad(item_data['labeled'],\n",
    "                                        ((pad_top, pad_bottom), (pad_left, pad_right)),\n",
    "                                        mode='constant')\n",
    "            item_data['labeled'] = np.expand_dims(item_data['labeled'], axis=0)\n",
    "\n",
    "            # Apply padding to raw (Torch tensor, using F.pad)\n",
    "            item_data['raw'] = pad(item_data['raw'],\n",
    "                                (pad_left, pad_top, pad_right, pad_bottom),\n",
    "                                padding_mode='constant')\n",
    "            \n",
    "\n",
    "        \n",
    "        return item_data\n",
    "    \n",
    "def reset_all_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    refs:\n",
    "        - https://discuss.pytorch.org/t/how-to-re-set-alll-parameters-in-a-network/20819/6\n",
    "        - https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        # - check if the current module has reset_parameters & if it's callabed called it on m\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()\n",
    "\n",
    "    # Applies fn recursively to every submodule see: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "    model.apply(fn=weight_reset)\n",
    "\n",
    "\n",
    "model_classes = {\n",
    "    \"FPN\": smp.FPN,\n",
    "    \"Unet\": smp.Unet,\n",
    "    \"MAnet\": smp.MAnet,\n",
    "    \"Linknet\": smp.Linknet,\n",
    "    \"PSPNet\": smp.PSPNet,\n",
    "    \"UnetPlusPlus\": smp.UnetPlusPlus\n",
    "}\n",
    "\n",
    "# ===================== #\n",
    "#   Run for 6 Folders   #\n",
    "# ===================== #\n",
    "for i in [1]:  # Loop 6 times for different dataset folders\n",
    "\n",
    "        # Store losses for plotting\n",
    "    training_loss_plot = []\n",
    "    validation_loss_plot = []\n",
    "\n",
    "    # ===================== #\n",
    "    #   Main Training Loop  #\n",
    "    # ===================== #\n",
    "    for model in model_list:\n",
    "        for backbone in backbone_list:\n",
    "            NNModel = model_classes[model]\n",
    "            nn_name = NNModel.__name__\n",
    "            model_to_run = 'Control'\n",
    "\n",
    "            print(f\"Training {nn_name} with backbone {backbone}\")\n",
    "\n",
    "            # ===================== #\n",
    "            #  Learning Rate Setup  #\n",
    "            # ===================== #\n",
    "            if os.path.exists(\"tuning_results.csv\"):\n",
    "                tuning_df = pd.read_csv(\"tuning_results.csv\")\n",
    "                try:\n",
    "                    learning_rate = tuning_df.loc[\n",
    "                        (tuning_df['model'] == nn_name) & \n",
    "                        (tuning_df['backbone'] == backbone)\n",
    "                    ].iloc[0]['lr']\n",
    "                except:\n",
    "                    learning_rate = DEFAULT_LEARNING_RATE\n",
    "            else:\n",
    "                learning_rate = DEFAULT_LEARNING_RATE\n",
    "\n",
    "            # ===================== #\n",
    "            #   Load Datasets       #\n",
    "            # ===================== #\n",
    "            train_dataset = TrainingDataset(\n",
    "                raw_folder=f\"Images/Train/Raw/\",\n",
    "                label_folder=f\"Images/Train/Labels/\"\n",
    "            )\n",
    "        \n",
    "            test_dataset = TrainingDataset(\n",
    "                raw_folder=f\"Images/Test/Raw/\",\n",
    "                label_folder=f\"Images/Test/Labels/\"\n",
    "            )\n",
    "        \n",
    "            # ===================== #\n",
    "            #   Create Dataloaders  #\n",
    "            # ===================== #\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            # ===================== #\n",
    "            #   Build Model/Paths   #\n",
    "            # ===================== #\n",
    "            num_classes = 2\n",
    "            base_model_path = f\"Models/{nn_name.lower()}_{backbone}/\"\n",
    "            os.makedirs(base_model_path, exist_ok=True)\n",
    "\n",
    "            if os.path.exists(\"training_results.csv\"):\n",
    "                training_results = pd.read_csv(\"training_results.csv\")\n",
    "            else:\n",
    "                training_results = pd.DataFrame(columns=[\n",
    "                    \"model\", \"backbone\", \"fold\", \"params\", \"lr\", \"f1\", \"accuracy\",\n",
    "                    \"precision\", \"recall\", \"sensitivity\", \"specificity\", \"iou\", \"iou_imagewise\",\n",
    "                    \"dice\", \"dice_imagewise\",\"train_s\"\n",
    "                ])\n",
    "\n",
    "\n",
    "            net = NNModel(backbone, classes=num_classes - 1)\n",
    "            total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "            # Multi-GPU\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                net = nn.DataParallel(net)\n",
    "\n",
    "            net.to(device)\n",
    "\n",
    "            # Loss & Optimizer\n",
    "            criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Keep track of best validation loss\n",
    "            best_val_loss = float(\"inf\")\n",
    "\n",
    "            # Keep track of best ioud\n",
    "            best_val_iou = 0.00\n",
    "\n",
    "            # ===================== #\n",
    "            #      Train Loop       #\n",
    "            # ===================== #\n",
    "            training_loss_epoch = []\n",
    "            validation_loss_epoch = []\n",
    "\n",
    "            # Initialize list to store epoch-wise metrics\n",
    "            results_list = []\n",
    "            t_start = time.time()\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "                # --- TRAIN ---\n",
    "                net.train()\n",
    "                batch_losses = []\n",
    "                for batch in train_loader:\n",
    "                    inputs = batch['raw'].to(device)\n",
    "                    labels = batch['labeled'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    batch_losses.append(loss.item())\n",
    "\n",
    "                epoch_train_loss = np.mean(batch_losses)\n",
    "                training_loss_epoch.append(epoch_train_loss)\n",
    "\n",
    "                # --- VALIDATION ---\n",
    "                val_batch_losses = []\n",
    "                all_outputs = []\n",
    "                all_labels = []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        inputs = batch['raw'].to(device)\n",
    "                        labels = batch['labeled'].to(device)\n",
    "\n",
    "                        preds = net(inputs)\n",
    "                        val_loss = criterion(preds, labels)\n",
    "                        val_batch_losses.append(val_loss.item())\n",
    "\n",
    "                        prob_mask = preds.sigmoid()\n",
    "                        pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "                        all_outputs.append(pred_mask.long())\n",
    "                        all_labels.append(labels.long())\n",
    "\n",
    "                epoch_val_loss = np.mean(val_batch_losses)\n",
    "                validation_loss_epoch.append(epoch_val_loss)\n",
    "\n",
    "                # Compute metrics\n",
    "                all_outputs = torch.cat(all_outputs, dim=0)\n",
    "                all_labels = torch.cat(all_labels, dim=0)\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(all_outputs, all_labels, mode=\"binary\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                accuracy_val = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                precision_val = smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                recall_val = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                f1_val = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                sensitivity_val = smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                specificity_val = smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                iou_val = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                iou_imagewise_val = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item()\n",
    "                dice_val = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "                dice_imagewise_val = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item()\n",
    "\n",
    "\n",
    "\n",
    "                # Append data to results list\n",
    "                results_list.append([\n",
    "                    epoch + 1, epoch_train_loss, epoch_val_loss, accuracy_val, precision_val, recall_val,\n",
    "                    f1_val, sensitivity_val, specificity_val, iou_val, iou_imagewise_val,\n",
    "                    dice_val, dice_imagewise_val,  # Added Dice metrics\n",
    "                    tp.sum().item(), fp.sum().item(), fn.sum().item(), tn.sum().item()\n",
    "                ])\n",
    "\n",
    "\n",
    "                print(f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "                    f\"Val Loss: {epoch_val_loss:.4f} | \"\n",
    "                    f\"F1: {f1_val:.3f} | IoU: {iou_val:.3f} | \"\n",
    "                    f\"DC: {dice_val:.3f} | \"\n",
    "                    f\"Image-wise IoU: {iou_imagewise_val:.3f} | \"\n",
    "                    f\"Image-wise DC: {dice_imagewise_val:.3f}\")\n",
    "\n",
    "\n",
    "                # Save best model\n",
    "                if iou_imagewise_val > best_val_iou:\n",
    "                    best_val_iou = iou_imagewise_val\n",
    "                    torch.save(net, os.path.join(base_model_path, \"best_model.pth\"))\n",
    "\n",
    "            # Convert list to DataFrame\n",
    "                columns = [\n",
    "                    \"epoch\", \"epoch_train_loss\", \"epoch_val_loss\", \"accuracy_val\", \"precision_val\", \"recall_val\",\n",
    "                    \"f1_val\", \"sensitivity_val\", \"specificity_val\", \"iou_val\", \"iou_imagewise_val\",\n",
    "                    \"dice_val\", \"dice_imagewise_val\",  # Added Dice metrics\n",
    "                    \"tp\", \"fp\", \"fn\", \"tn\"\n",
    "                ]\n",
    "\n",
    "\n",
    "            df_results = pd.DataFrame(results_list, columns=columns)\n",
    "\n",
    "            # Define the save path\n",
    "            save_path = os.path.join(base_model_path, \"epochs_report.csv\")\n",
    "\n",
    "            # Save to CSV\n",
    "            df_results.to_csv(save_path, index=False)\n",
    "\n",
    "            print(\"Finished Training\")\n",
    "\n",
    "            # ===================== #\n",
    "            #   Final Test Metrics  #\n",
    "            # ===================== #\n",
    "            net = torch.load(os.path.join(base_model_path, \"net.pth\"))\n",
    "            net.eval()\n",
    "\n",
    "            all_outputs = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    inputs = batch['raw'].to(device)\n",
    "                    labels = batch['labeled'].to(device)\n",
    "\n",
    "                    preds = net(inputs)\n",
    "                    prob_mask = preds.sigmoid()\n",
    "                    pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "                    all_outputs.append(pred_mask.long())\n",
    "                    all_labels.append(labels.long())\n",
    "\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "            all_labels = torch.cat(all_labels, dim=0)\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(all_outputs, all_labels, mode=\"binary\")\n",
    "\n",
    "            final_accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_precision = smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_sensitivity = smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_specificity = smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
    "            final_iou_imagewise = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item()\n",
    "            final_dice = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()  # Regular Dice\n",
    "            final_dice_imagewise = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item()  # Image-wise Dice\n",
    "\n",
    "            t_end       = time.time()\n",
    "            total_seconds = (t_end - t_start)\n",
    "\n",
    "            print(\"\\n--- Final Test Metrics (Best Model) ---\")\n",
    "            print(f\"Accuracy:     {final_accuracy:.4f}\")\n",
    "            print(f\"Precision:    {final_precision:.4f}\")\n",
    "            print(f\"Recall:       {final_recall:.4f}\")\n",
    "            print(f\"F1:           {final_f1:.4f}\")\n",
    "            print(f\"Sensitivity:  {final_sensitivity:.4f}\")\n",
    "            print(f\"Specificity:  {final_specificity:.4f}\")\n",
    "            print(f\"IoU:          {final_iou:.4f}\")\n",
    "            print(f\"IoU-Imagewise:{final_iou_imagewise:.4f}\")\n",
    "            print(f\"Dice:         {final_dice:.4f}\")  # Print regular Dice\n",
    "            print(f\"Dice-Imagewise:{final_dice_imagewise:.4f}\")  # Print image-wise Dice\n",
    "\n",
    "            # ===================== #\n",
    "            #  Save Results to CSV  #\n",
    "            # ===================== #\n",
    "            new_row = pd.DataFrame({\n",
    "                \"model\": [nn_name],\n",
    "                \"backbone\": [backbone],\n",
    "                \"fold\": [i],\n",
    "                \"params\": [total_params],\n",
    "                \"lr\": [learning_rate],\n",
    "                \"accuracy\": [final_accuracy],\n",
    "                \"precision\": [final_precision],\n",
    "                \"recall\": [final_recall],\n",
    "                \"f1\": [final_f1],\n",
    "                \"sensitivity\": [final_sensitivity],\n",
    "                \"specificity\": [final_specificity],\n",
    "                \"iou\": [final_iou],\n",
    "                \"iou_imagewise\": [final_iou_imagewise],\n",
    "                \"dice\": [final_dice],  # Added regular Dice\n",
    "                \"dice_imagewise\": [final_dice_imagewise],  # Added image-wise Dice\n",
    "                \"Train_s\": [ total_seconds]\n",
    "            })\n",
    "            training_results = pd.concat([training_results, new_row], ignore_index=True)\n",
    "            training_results.to_csv(\"training_results7.csv\", index=False)\n",
    "\n",
    "\n",
    "            # ===================== #\n",
    "            #  Final Prediction Gen #\n",
    "            # ===================== #\n",
    "            # Create paths to store outputs\n",
    "            pred_path = os.path.join(base_model_path, \"Predictions\")\n",
    "            comp_path = os.path.join(base_model_path, \"Comparisons\")\n",
    "            if not os.path.exists(pred_path):\n",
    "                os.makedirs(pred_path)\n",
    "            if not os.path.exists(comp_path):\n",
    "                os.makedirs(comp_path)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data_dict in test_loader:\n",
    "                    # Decide how to name output image\n",
    "                    if model_to_run in ['Triple', 'Random']:\n",
    "                        image_name = data_dict['categoryname'][0]\n",
    "                    else:  # 'Control'\n",
    "                        image_name = data_dict['filename'][0]\n",
    "\n",
    "                    data = data_dict['raw'].clone().to(device)\n",
    "                    labels = data_dict['labeled'].clone().to(device)\n",
    "\n",
    "                    outputs = net(data)\n",
    "                    prob_mask = outputs.sigmoid()\n",
    "                    outputs = (prob_mask > 0.5).float()\n",
    "\n",
    "                    # Convert prediction to CPU numpy\n",
    "                    pred = outputs.to('cpu')\n",
    "                    p = pred.numpy().squeeze()\n",
    "\n",
    "                    # --- Unpad/crop dynamically using recorded padding ---\n",
    "                    pad_info = data_dict['pad']\n",
    "                    pt = int(pad_info['top'])\n",
    "                    pb = int(pad_info['bottom'])\n",
    "                    pl = int(pad_info['left'])\n",
    "                    pr = int(pad_info['right'])\n",
    "\n",
    "                    if (pt + pb + pl + pr) > 0:\n",
    "                        p_cropped = p[pt:p.shape[0]-pb, pl:p.shape[1]-pr]\n",
    "                    else:\n",
    "                        p_cropped = p\n",
    "\n",
    "                    p_cropped = p_cropped.astype('f4')\n",
    "\n",
    "                    # Save prediction as image\n",
    "                    p_as_image = nparray2image(p_cropped, image_name, pred_path)\n",
    "\n",
    "                    # Example cropping/padding fix:\n",
    "                    # left=4, top=13, right=5, bottom=13 (adjust as needed)\n",
    "                    #p_as_image = p_as_image[13:-13, 4:-5, :]\n",
    "                    p_as_image = p_as_image.astype('f4')\n",
    "\n",
    "                    # Prepare a 3-panel comparison image\n",
    "                    img_raw_tens = data_dict['rawimg']\n",
    "                    img_raw = tens2numpy(img_raw_tens)\n",
    "                    img_lab_tens = data_dict['labelimg']\n",
    "                    img_lab = tens2numpy(img_lab_tens)\n",
    "                    p_comp = pred2comp(p_as_image, img_raw, img_lab, image_name, comp_path)\n",
    "\n",
    "\n",
    "            # ===================== #\n",
    "            #  Append for plotting  #\n",
    "            # ===================== #\n",
    "            training_loss_plot.append(training_loss_epoch)\n",
    "            validation_loss_plot.append(validation_loss_epoch)\n",
    "\n",
    "            # ===================== #\n",
    "            #   Plot Loss Curves    #\n",
    "            # ===================== #\n",
    "            plt.plot(training_loss_epoch, label=\"Training Loss\")\n",
    "            plt.plot(validation_loss_epoch, label=\"Validation Loss\")\n",
    "            plt.legend(loc='best')\n",
    "            plt.savefig(os.path.join(base_model_path, f\"{nn_name.lower()}_{backbone}_loss.png\"))\n",
    "            plt.clf()\n",
    "\n",
    "print(\"All training done.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "34718bd5e5ff0ec954d04da905bd4ff4d511a0bfb6f9a1aa7037f750dbb407a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
